
# DeepSeek R1 Distill Qwen 1.5B: Harnessing the Power of Transformers

This repository hosts the implementation of the **DeepSeek R1 Distill Qwen 1.5B** model, showcasing its capabilities in natural language generation and efficient task execution using the **Transformers** library.

## Overview

The notebook demonstrates:
- Loading the **DeepSeek R1 Distill Qwen 1.5B** model from Hugging Face.
- Prompt-based AI generation using Transformer models.
- Generating a Python function to compute the Fibonacci sequence efficiently with O(n) time complexity.

## Key Features

- **Model**: DeepSeek R1 Distill Qwen 1.5B, an optimized Transformer model for causal language modeling.
- **Usage**: Tokenization and generation of user-defined prompts.
- **Efficiency**: Generates responses with context-based logic and code examples.

## Installation and Setup

1. **Clone this repository**:
   ```bash
   git clone https://github.com/<your-username>/deepseek_r1_distill_qwen1_5B_transformers.git
   cd deepseek_r1_distill_qwen1_5B_transformers


## Install dependencies: Ensure you have Python and transformers installed.

bash
Copy
Edit
pip install transformers

Run the notebook: Open the notebook in your preferred environment and execute the cells sequentially.

Model and Tokenizer
The model and tokenizer are fetched from Hugging Face:

Model: DeepSeek-R1-Distill-Qwen-1.5B
Device: Optimized for GPU execution.

Example Prompt
python
Copy
Edit
prompt = "Create a Python function to calculate Fibonacci sequence with O(n) time complexity"

The AI processes this prompt and generates an optimized solution leveraging efficient algorithms.

Outputs
Generated Code: Python code snippet implementing the desired functionality.
Decoding: The generated response is decoded and displayed for review.

Sample Use Case
The notebook provides a step-by-step explanation of generating the Fibonacci sequence efficiently using an iterative method, demonstrating:

Prompt input.
AI-generated response with detailed logic and implementation.
Evaluation and testing of the generated solution.


# reponse 

'
# DeepSeek R1 Distill Qwen 1.5B: Harnessing the Power of Transformers

This repository hosts the implementation of the **DeepSeek R1 Distill Qwen 1.5B** model, showcasing its capabilities in natural language generation and efficient task execution using the **Transformers** library.

## Overview

The notebook demonstrates:
- Loading the **DeepSeek R1 Distill Qwen 1.5B** model from Hugging Face.
- Prompt-based AI generation using Transformer models.
- Generating a Python function to compute the Fibonacci sequence efficiently with O(n) time complexity.

## Key Features

- **Model**: DeepSeek R1 Distill Qwen 1.5B, an optimized Transformer model for causal language modeling.
- **Usage**: Tokenization and generation of user-defined prompts.
- **Efficiency**: Generates responses with context-based logic and code examples.

## Installation and Setup

1. **Clone this repository**:
   ```bash
   git clone https://github.com/<your-username>/deepseek_r1_distill_qwen1_5B_transformers.git
   cd deepseek_r1_distill_qwen1_5B_transformers


## Install dependencies: Ensure you have Python and transformers installed.

bash
Copy
Edit
pip install transformers

Run the notebook: Open the notebook in your preferred environment and execute the cells sequentially.

Model and Tokenizer
The model and tokenizer are fetched from Hugging Face:

Model: DeepSeek-R1-Distill-Qwen-1.5B
Device: Optimized for GPU execution.

Example Prompt
python
Copy
Edit
prompt = "Create a Python function to calculate Fibonacci sequence with O(n) time complexity"

The AI processes this prompt and generates an optimized solution leveraging efficient algorithms.

Outputs
Generated Code: Python code snippet implementing the desired functionality.
Decoding: The generated response is decoded and displayed for review.

Sample Use Case
The notebook provides a step-by-step explanation of generating the Fibonacci sequence efficiently using an iterative method, demonstrating:

Prompt input.
AI-generated response with detailed logic and implementation.
Evaluation and testing of the generated solution.'
